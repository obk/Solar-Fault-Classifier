{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model = load_model(\"solar_fault_classifier.h5\")\n",
    "\n",
    "# Define function to preprocess images\n",
    "def preprocess_image(img_path, target_size=(150, 150)):\n",
    "    img = image.load_img(img_path, target_size=target_size)\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    img_array = img_array / 255.0  # Normalize\n",
    "    return img_array\n",
    "\n",
    "# Define class labels\n",
    "class_labels = [\"Bypass_Diode_activated\", \"Junction_box\", \n",
    "               \"Multicell_hotspot\", \"Single_cell_hotspot\"]\n",
    "\n",
    "# Set path to test images directory\n",
    "test_dir = \"images_classed\"\n",
    "\n",
    "# Initialize metrics\n",
    "correct = 0\n",
    "incorrect = 0\n",
    "actual_labels = []\n",
    "predicted_labels = []\n",
    "results = []\n",
    "\n",
    "# Process images and make predictions\n",
    "for class_name in os.listdir(test_dir):\n",
    "    class_path = os.path.join(test_dir, class_name)\n",
    "    \n",
    "    if os.path.isdir(class_path):\n",
    "        for img_name in os.listdir(class_path):\n",
    "            img_path = os.path.join(class_path, img_name)\n",
    "            \n",
    "            # Preprocess and predict\n",
    "            img_array = preprocess_image(img_path)\n",
    "            predictions = model.predict(img_array)\n",
    "            predicted_class = np.argmax(predictions)\n",
    "            predicted_label = class_labels[predicted_class]\n",
    "            \n",
    "            # Store results\n",
    "            actual_labels.append(class_name)\n",
    "            predicted_labels.append(predicted_label)\n",
    "            results.append((img_name, class_name, predicted_label))\n",
    "            \n",
    "            # Update counts\n",
    "            if class_name == predicted_label:\n",
    "                correct += 1\n",
    "            else:\n",
    "                incorrect += 1\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = correct / (correct + incorrect) * 100\n",
    "report = classification_report(actual_labels, predicted_labels, \n",
    "                              target_names=class_labels, output_dict=True)\n",
    "cm = confusion_matrix(actual_labels, predicted_labels, labels=class_labels)\n",
    "\n",
    "# Class-wise accuracy calculation\n",
    "class_counts = {cls: actual_labels.count(cls) for cls in class_labels}\n",
    "class_correct = {cls: sum(1 for a, p in zip(actual_labels, predicted_labels) \n",
    "                        if a == cls and p == cls) for cls in class_labels}\n",
    "class_accuracy = {cls: f\"{(class_correct[cls]/class_counts[cls])*100:.2f}%\" \n",
    "                 for cls in class_labels}\n",
    "\n",
    "# Print detailed report\n",
    "print(\"================= Comprehensive Report =================\")\n",
    "print(f\"Total Images Tested: {correct + incorrect}\")\n",
    "print(f\"Overall Accuracy: {accuracy:.2f}%\")\n",
    "print(\"\\nClass-wise Performance:\")\n",
    "print(f\"{'Class':<25} {'Accuracy':<10} {'Precision':<10} {'Recall':<10} {'F1-Score':<10}\")\n",
    "for cls in class_labels:\n",
    "    print(f\"{cls:<25} {class_accuracy[cls]:<10} \"\n",
    "          f\"{report[cls]['precision']*100:.2f}%   \"\n",
    "          f\"{report[cls]['recall']*100:.2f}%    \"\n",
    "          f\"{report[cls]['f1-score']*100:.2f}%\")\n",
    "\n",
    "# Create visualizations\n",
    "plt.figure(figsize=(18, 15))\n",
    "\n",
    "# 1. Confusion Matrix\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.title('Confusion Matrix', pad=20)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('Actual Labels')\n",
    "\n",
    "# 2. Metric Comparison\n",
    "plt.subplot(2, 2, 2)\n",
    "metrics = ['precision', 'recall', 'f1-score']\n",
    "for metric in metrics:\n",
    "    plt.plot(class_labels, [report[cls][metric] for cls in class_labels], \n",
    "             marker='o', linewidth=2, markersize=8, label=metric.capitalize())\n",
    "plt.title('Performance Metrics Comparison', pad=20)\n",
    "plt.ylim(0, 1.1)\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Class Accuracy\n",
    "plt.subplot(2, 2, 3)\n",
    "acc_values = [float(class_accuracy[cls].strip('%')) for cls in class_labels]\n",
    "plt.barh(class_labels, acc_values, color=['#2ecc71', '#3498db', '#f1c40f', '#e74c3c'])\n",
    "plt.xlabel('Accuracy (%)', labelpad=10)\n",
    "plt.title('Class-wise Accuracy Rates', pad=20)\n",
    "plt.xlim(0, 110)\n",
    "for i, v in enumerate(acc_values):\n",
    "    plt.text(v + 1, i, f\"{v:.1f}%\", color='black', va='center')\n",
    "\n",
    "# 4. Prediction Distribution\n",
    "plt.subplot(2, 2, 4)\n",
    "wrong_counts = [class_counts[cls] - class_correct[cls] for cls in class_labels]\n",
    "plt.bar(class_labels, class_correct.values(), color='#2ecc71', label='Correct')\n",
    "plt.bar(class_labels, wrong_counts, bottom=class_correct.values(),\n",
    "        color='#e74c3c', label='Incorrect')\n",
    "plt.title('Prediction Distribution', pad=20)\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.ylabel('Number of Images')\n",
    "\n",
    "plt.tight_layout(pad=3.0)\n",
    "plt.show()\n",
    "# Sample predictions visualization\n",
    "plt.figure(figsize=(15, 8))\n",
    "for i, (img_name, actual, predicted) in enumerate(results[:10]):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    img_path = os.path.join(test_dir, actual, img_name)\n",
    "    img = image.load_img(img_path, target_size=(150, 150))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    color = 'green' if actual == predicted else 'red'\n",
    "    plt.title(f\"Actual: {actual}\\nPred: {predicted}\", color=color)\n",
    "plt.suptitle('Sample Predictions (Green=Correct, Red=Incorrect)', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
